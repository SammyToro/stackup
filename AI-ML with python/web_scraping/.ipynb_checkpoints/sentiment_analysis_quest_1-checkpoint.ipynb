{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e96f0df1-5382-43cc-934b-f25477818846",
   "metadata": {},
   "source": [
    "# AI/ML with Python: Web Scraping & Sentiment Analysis\n",
    "## Data Extraction and Preprocessing through Webscraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ba5a2c",
   "metadata": {},
   "source": [
    "### Step 3 - Fetching Raw HTML Web Content\n",
    "\n",
    "We start by accessing and obtaining the raw HTML of a webpage. Turning this into a comprehensible string format enables subsequent processing and analysis activities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220560b7-319e-45c7-9237-0f1ef42db374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package used to work with URLs\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# Link to the website\n",
    "url = \"https://quotes.toscrape.com/\"\n",
    "\n",
    "# Opening the URL and reading it\n",
    "page = urlopen(url)\n",
    "html_bytes = page.read()\n",
    "\n",
    "# Taking raw web content in bytes format and converting it into a readable string format using the UTF-8 encoding\n",
    "html = html_bytes.decode(\"utf-8\")\n",
    "\n",
    "# Print raw HTML of the webpage\n",
    "html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6da76ee-192f-4103-a282-b0561cb7ec48",
   "metadata": {},
   "source": [
    "### Step 4 - Understanding Regular Expression in Webscraping\n",
    "\n",
    "In this exercise, we'll continue from where we left off, after getting the raw HTML source code of the webpage. We proceed to import `re`, the module that supports regular expressions. We will be extracting the title tag from this webpage and printing it out below, which in this case is \"Quotes to Scrape\".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b33a41-8ad1-4ef3-b7b5-ac0bc71756df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "pattern = \"<title.*?>.*?</title.*?>\"\n",
    "match_results = re.search(pattern, html, re.IGNORECASE)\n",
    "title = match_results.group()\n",
    "\n",
    "title = re.sub(\"<.*?>\", \"\", title)\n",
    "\n",
    "title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c63f58-8448-40d6-bc56-c33e65ebad52",
   "metadata": {},
   "source": [
    "We begin off with the pattern that we will be analyzing, then from there use the pattern to search for the tags labeled as \"title\". We begin with the pattern that we will be analyzing, then from there use the pattern to search for the tags labeled as \"title\". \n",
    "\n",
    "The `<title.*?>` matches the opening `<title>` tag. The `.*?` part ensures that it matches any character (represented by `.`) any number of times (represented by `*`), but it will stop matching once it finds the first closing `>` character (represented by `?`). At its core, regex employs specialized symbols to identify patterns within text strings.\n",
    "\n",
    "It then utilizes the pattern to search for the first HTML tag that has `<title>` in it, ignoring case sensitivity (it doesn't care if it is uppercase or lowercase). This then retrieves the HTML tags from the `match_results`and removes the symbols `<.*?>`, before finally retrieving the word within the title.\n",
    "\n",
    "Pretty neat! A search online will help if you're uncertain about what pattern you can use, then you can proceed to use the remaining code to extract other relevant information you require."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903aa170-25e7-4dba-8158-3d287a87f1f4",
   "metadata": {},
   "source": [
    "### Step 5 - Web Scraping with BeautifulSoup\n",
    "\n",
    "While regex might serve well for basic web scraping tasks, it's often not the ideal choice for more intricate or expansive web scraping projects. \n",
    "\n",
    "BeautifulSoup, a widely-used Python library for web scraping, stands out because of its user-friendly and versatile features for parsing and traversing HTML and XML content. \n",
    "\n",
    "Key benefits include its adaptability in parsing and its advanced search capabilities. Let's delve into an example to showcase the simplicity of using BeautifulSoup!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7165f5f-e21d-4e3d-9f2d-8e210b710ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to install BeautifulSoup4 first\n",
    "# python -m pip install beautifulsoup4 \n",
    "# try pip3 if pip is not found\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "text = soup.find_all(\"span\", class_=\"text\")\n",
    "\n",
    "for obj in text:\n",
    "    print(obj.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57c18dd-3f57-4f44-9870-a7201cff89ca",
   "metadata": {},
   "source": [
    "In this case, we can see that by running this code, we are extracting all the actual quotes from famous people. In this case, BeautifulSoup can be used to parse the raw HTML files and find all `<span>` tags with \"text\" as their class. This stores all the span elements within a list, which can then be extracted by using the `get_text()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f7473e-e121-48f9-810a-fb1c439c871c",
   "metadata": {},
   "source": [
    "### Step 6 - Automating Tasks with Selenium\n",
    "\n",
    "We'll learn how to make use of Selenium to automate a login process using a dummy website that you can visit by clicking here <href>https://the-internet.herokuapp.com/login<href/>. \n",
    "\n",
    "This simple website allows us to learn how to interact with elements, autofill fields, and click on buttons to login without you having to manually do so! We start by installing Selenium package. \n",
    "\n",
    "We then proceed to import both Webdriver and keys which are necessary to open up a new browser and interact with input fields, as you will see in a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c81fc6-b01e-484f-9ff4-6aaecae374e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input 'pip install selenium' or 'pip3 install selenium' into your terminal\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c99c115-03cd-4926-8cf7-7e8cbd1ebca5",
   "metadata": {},
   "source": [
    "With that in place, we then go on next to open up the web browser and update both the 'username' and 'password' fields. After running the cell below, you should see a new browser window opened!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff817f7b-cd2a-4a56-8106-4b640827c923",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Automated Tasks with Selenium\n",
    "\n",
    "link = 'https://the-internet.herokuapp.com/login'\n",
    "\n",
    "# Initializing webdriver and getting the link\n",
    "# using Chrome as primary browser. However if you are using another browser, you will have to download it from the Microsoft Edge Webdriver page\n",
    "driver = webdriver.Chrome()\n",
    "driver.get((link))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb37c0c-58be-4dff-8dc5-16206efe8f2e",
   "metadata": {},
   "source": [
    "With the browser opened, we can go ahead and begin interacting with the input fields **using code**! \n",
    "\n",
    "Before updating the fields, we have to first locate the elements that indicate where the fields are. \n",
    "\n",
    "As previously covered, we can either make use of BeautifulSoup or opening up developer tools to study the raw HTML web content. The code below demonstates how we can do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35cd245-1dca-46cf-8ea0-fd604eb0fb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_html = urlopen(link)\n",
    "login_page = BeautifulSoup(raw_html, \"html.parser\")\n",
    "login_page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a5536c-d105-49e2-8ae8-3821036e4d46",
   "metadata": {},
   "source": [
    "From the output above, we can easily find out that the elements that are linked to the username and password fields are both input tags with id \"username\" and \"password\" respectively. \n",
    "\n",
    "This is relatively easy to find because it is just an example, but in more complex websites it is better to locate them via developer consoles.\n",
    "\n",
    "To do that, we make use of the `find_element` tag to locate each input field by its id as shown below, by importing the `by` class.\n",
    "\n",
    "We then send the keys to the input field by using the `keys` class. By running the script below, you should see **both fields within the automated browser** that you just opened get **automatically updated**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eba95a6-6add-49df-add3-81cf4106e48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-establishing the password and username \n",
    "# Note that if you are doing this for your own project, ensure you are securing your credentials to prevent misuse\n",
    "\n",
    "user = 'tomsmith'\n",
    "pw = 'SuperSecretPassword!'\n",
    "\n",
    "# Get the element of both username and password fields\n",
    "username_input = driver.find_element(By.ID, \"username\")\n",
    "password_input = driver.find_element(By.ID, \"password\")\n",
    "\n",
    "# Input the credentials (the credentials for the-internet are usually provided on the page)\n",
    "username_input.send_keys(user)\n",
    "password_input.send_keys(pw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23862504-199f-4fa7-beaf-2cad59e9a8d1",
   "metadata": {},
   "source": [
    "Now that we've updated both fields, the next thing we'll get Selenium to do is to **click the login button**. This time, we'll make use of `CSS Selector` because this button does not have a id associated to it. \n",
    "\n",
    "In short, `CSS selector` just identifies elements on a web page based on their cascading style sheet (CSS) properties. They are used to select the HTML elements with that specific css tags. \n",
    "\n",
    "In this case, we are using an attribute selector, which selects an element based on the presence or value of a given attribute. In this case, it selects the `button` element with the type 'submit'.\n",
    "\n",
    "If you'll like to check on more on CSS selectors, visit this link here: <href>https://saucelabs.com/resources/blog/selenium-tips-css-selectors<href/> \n",
    "\n",
    "Run the code below to witness it in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a038ad4-78d0-4eb1-8da6-f984a48dc737",
   "metadata": {},
   "outputs": [],
   "source": [
    "login_button = driver.find_element(By.CSS_SELECTOR, \"button[type='submit']\")\n",
    "login_button.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c5f6f3-c65a-4c73-a40d-2b5171ae994e",
   "metadata": {},
   "source": [
    "And...you should be logged in! In this short excercise, we have successfully harnessed the ability of Selenium to interact with elements for web scraping. \n",
    "\n",
    "For websites that use JavaScript heavily to load the content dynamically, Selenium can circumvent that by triggering elements to extract data from that. \n",
    "\n",
    "Remember that you should read and understood all terms and conditions of a site to ensure that you don't violate any ethical and legal constraints!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9d95f3-9e86-4b3c-b32b-295ab3370afd",
   "metadata": {},
   "source": [
    "### Step 7 - Let's Ace Your Submissions! Preparing Your Submission!\n",
    "Follow the instructions under Step 7 to complete this quest. We have provided instructions below to guide you along the way, so please refer to previous steps or check the web if you are uncertain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a46575b1-d07a-48f4-8870-932715456a8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Importing all the necessary packages\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m webdriver\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mby\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m By\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbs4\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'selenium'"
     ]
    }
   ],
   "source": [
    "# Importing all the necessary packages\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# Opening the link using Webdriver\n",
    "link = \"https://quotes.toscrape.com/\"\n",
    "driver = webdriver.Chrome()\n",
    "driver.get((link))\n",
    "# Navigating to the next page by clicking next\n",
    "next_button = driver.find_element(By.CSS_SELECTOR, \"ul>li.next>a\")\n",
    "next_button.click()\n",
    "# Get the link of the current page and opening\n",
    "current_page_url = driver.current_url\n",
    "\n",
    "# Taking raw web content in bytes format and converting it into a readable string format using the UTF-8 encoding\n",
    "page = urlopen(current_page_url)\n",
    "html_bytes = page.read()\n",
    "html = html_bytes.decode(\"utf-8\")\n",
    "\n",
    "# Use beautifulSoup package to parse and read them\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "text = soup.find_all(\"span\", class_=\"text\")\n",
    "\n",
    "# Print out each quote\n",
    "for obj in text:\n",
    "    print(obj.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b480f9c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
